name: Process Novel Folder to DOCX

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  process-novel:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: pip install chardet python-docx

      - name: Process novel folder
        run: |
          import os
          import zipfile
          import json
          import re
          import shutil
          import traceback
          from chardet import detect
          from docx import Document
          from docx.shared import Pt
          from docx.enum.text import WD_PARAGRAPH_ALIGNMENT

          NOVEL_DIR = "novel"
          OUTPUT_DIR = "output"

          # Common encodings to try (in order of preference)
          COMMON_ENCODINGS = [
              'utf-8', 'gb18030', 'big5', 'gbk', 'gb2312',
              'utf-16', 'iso-8859-1'
          ]

          os.makedirs(OUTPUT_DIR, exist_ok=True)

          # --- Utility Functions (Encoding, Chinese Numerals) ---
          def detect_encoding(file_path):
              try:
                  with open(file_path, 'rb') as f:
                      rawdata = f.read(4096);
                      if not rawdata: return None
                      result = detect(rawdata)
                      return result['encoding'] if result['encoding'] and result['confidence'] > 0.7 else None
              except Exception: return None

          def read_file_with_fallback(file_path):
              detected_enc = detect_encoding(file_path); tried_encodings = set()
              if detected_enc:
                  try:
                      norm_enc = detected_enc.lower();
                      with open(file_path, 'r', encoding=norm_enc) as f: return f.read()
                  except (UnicodeDecodeError, LookupError): pass
                  finally: tried_encodings.add(detected_enc.lower()) if detected_enc else None
              for enc in COMMON_ENCODINGS:
                  norm_enc = enc.lower();
                  if norm_enc in tried_encodings: continue
                  try:
                      with open(file_path, 'r', encoding=norm_enc) as f: print(f"Read {os.path.basename(file_path)} with fallback '{norm_enc}'"); return f.read()
                  except (UnicodeDecodeError, LookupError): tried_encodings.add(norm_enc)
                  except FileNotFoundError: raise ValueError(f"File not found: {file_path}")
                  except Exception: tried_encodings.add(norm_enc)
              raise ValueError(f"Could not decode {os.path.basename(file_path)} with tried encodings ({', '.join(tried_encodings)}).")

          def chinese_to_arabic(chinese_num_str):
              if not isinstance(chinese_num_str, str): return int(chinese_num_str) if isinstance(chinese_num_str, (int, float)) else None
              s = chinese_num_str.strip();
              if not s: return None
              if s.isdigit(): return int(s)
              if not all(c in '零一二三四五六七八九十百千万億' for c in s): return None
              n_map={'零':0,'一':1,'二':2,'三':3,'四':4,'五':5,'六':6,'七':7,'八':8,'九':9}; u_map={'十':10,'百':100,'千':1000,'万':10000,'億':100000000}
              t=0; c=0; sec=0; u=1;
              if s=='十': return 10
              if s.startswith('十'): s='一'+s
              for char in s:
                  if char in n_map: c=n_map[char]
                  elif char in u_map:
                      cur_u=u_map[char];
                      if cur_u in [10000,100000000]: sec+=(c if c>0 else (1 if u>1 else 0))*u; t+=sec*cur_u; sec=0; c=0; u=1
                      else: sec+=(c if c>0 else 1)*cur_u; c=0; u=cur_u
                  else: return None
              sec+=c*(u if u>1 and c>0 and sec==0 else 1); t+=sec
              return t
          # --- End Utility Functions ---


          # --- parse_novel_content (Includes Sorting) ---
          def parse_novel_content(content):
              """Parses the raw text content into metadata and chapters, then sorts chapters."""
              novel = {
                  "metadata": {"name": "未知", "author": "未知", "status": "未知", "introduction": "無"},
                  "chapters": []
              }
              # Metadata Extraction Patterns...
              metadata_patterns = [
                  re.compile(r'《(?P<name>.+?)》\s*作者：(?P<author>.+?)\n.*?(?:內容|内容)簡介：\s*(?P<intro>.*?)\n(?:作品(?:狀態|状态)：\s*(?P<status>.+?)\n)?', re.DOTALL | re.IGNORECASE),
                  re.compile(r'『(?P<name>.+?)/作者:(?P<author>.+?)』.*?『狀態:更新到:(?P<status>.+?)』.*?『(?:內容|内容)簡介:(?P<intro>.+?)』', re.DOTALL | re.IGNORECASE),
                  re.compile(r'^(?P<name>[^\n]+)\n(?:作者：?|作\s*者：?)\s*(?P<author>[^\n]+)', re.MULTILINE | re.IGNORECASE),
                  re.compile(r'(?:書名|书名|名稱|名称)\s*[:：]\s*(?P<name>[^\n]+).*?(?:作者)\s*[:：]\s*(?P<author>[^\n]+).*?(?:(?:內容|内容)簡介?|简介)\s*[:：]\s*(?P<intro>.*?)(?=\n(?:[^\n]+[:：])|\Z).*?(?:(?:狀態|状态))\s*[:：]\s*(?P<status>[^\n]+)?', re.DOTALL | re.IGNORECASE)
              ]
              meta_match = None; content_start_index = 0
              for i, pattern in enumerate(metadata_patterns):
                  meta_match = pattern.search(content)
                  if meta_match:
                      meta_dict = meta_match.groupdict()
                      novel["metadata"]["name"] = meta_dict.get("name", "未知").strip()
                      novel["metadata"]["author"] = meta_dict.get("author", "未知").strip()
                      novel["metadata"]["status"] = meta_dict.get("status", "未知").strip() if meta_dict.get("status") else "未知"
                      novel["metadata"]["introduction"] = meta_dict.get("intro", "無").strip() if meta_dict.get("intro") else "無"
                      content_start_index = meta_match.end(); break
              else: print("Warning: No metadata patterns matched.")
              # Chapter Content Area Identification...
              chapter_content_area = content[content_start_index:]
              chapter_start_markers = [r'------章節內容開始-------', r'正文卷', r'VIP卷', r'作品相关', r'正文']
              marker_found = False
              for marker in chapter_start_markers:
                   marker_match = re.search(marker, chapter_content_area, re.IGNORECASE)
                   if marker_match: chapter_content_area = chapter_content_area[marker_match.end():]; marker_found = True; break
              chapter_content_area = chapter_content_area.strip()
              # Chapter Parsing Pattern...
              chapter_pattern = re.compile(
                  r'^[ \t]*(?:(?:(?:正文[ \t]*)?第[ \t]*([零一二三四五六七八九十百千万]+|[0-9]+)[ \t]*[章节卷集篇][ \t]*)|(Chapter[ \t]*([0-9]+)[ \t]*))([^\n]*)?\n([\s\S]*?)(?=\n[ \t]*(?:(?:正文[ \t]*)?第[ \t]*(?:[零一二三四五六七八九十百千万]+|[0-9]+)[ \t]*[章节卷集篇]|Chapter[ \t]*[0-9]+)|\Z)',
                  re.MULTILINE | re.IGNORECASE
              )
              found_chapters = []; last_match_end = 0
              for match in chapter_pattern.finditer(chapter_content_area):
                  chinese_num = match.group(1); english_num = match.group(3); title = (match.group(4) or "").strip(); text = match.group(5)
                  chapter_num_str = chinese_num or english_num or f"sequential_{len(found_chapters)+1}"
                  if text: text = re.sub(r'\r\n','\n',re.sub(r'\n{3,}','\n\n',text)).strip()
                  else: text = ""
                  numeric_chapter_num = chinese_to_arabic(chapter_num_str)
                  found_chapters.append({
                      "chapter_number_original": chapter_num_str, "chapter_number_numeric": numeric_chapter_num,
                      "chapter_title": title if title else f"Chapter {chapter_num_str}", "content": text
                  })
                  last_match_end = match.end()
              # Preface Handling...
              first_match_start = chapter_pattern.search(chapter_content_area).start() if chapter_pattern.search(chapter_content_area) else len(chapter_content_area)
              potential_preface = chapter_content_area[:first_match_start].strip()
              if potential_preface and len(potential_preface) > 50:
                   print(f"Preface found ({len(potential_preface)} chars). Adding as Chapter 0.")
                   found_chapters.append({"chapter_number_original": "0", "chapter_number_numeric": 0, "chapter_title": "序章/前言", "content": potential_preface})
              elif not found_chapters and chapter_content_area:
                  print("Warning: No chapters matched. Saving all as Chapter 1.")
                  found_chapters.append({"chapter_number_original": "1", "chapter_number_numeric": 1, "chapter_title": "全文", "content": chapter_content_area})
              # Sorting Logic...
              def sort_key(chap): num = chap.get('chapter_number_numeric'); return -1 if num == 0 else (float('inf') if num is None else num)
              found_chapters.sort(key=sort_key)
              novel["chapters"] = found_chapters
              return novel
          # --- End parse_novel_content ---


          # --- save_novel_as_docx ---
          def save_novel_as_docx(novel_name, novel_data, output_base_dir):
              """Save the entire novel as a DOCX file with formatted content."""
              sanitized_name = re.sub(r'[<>:"/\\|?*]', '_', novel_name)[:200]
              output_file_path = os.path.join(output_base_dir, f"{sanitized_name}.docx")
              
              try:
                  doc = Document()
                  
                  # Add title
                  title = doc.add_paragraph(novel_data["metadata"]["name"])
                  title.style = doc.styles['Title']
                  title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
                  
                  # Add author
                  author = doc.add_paragraph(f"作者: {novel_data['metadata']['author']}")
                  author.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
                  
                  # Add status
                  status = doc.add_paragraph(f"状态: {novel_data['metadata']['status']}")
                  status.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
                  
                  # Add introduction
                  doc.add_paragraph("简介:", style='Heading2')
                  intro = doc.add_paragraph(novel_data["metadata"]["introduction"])
                  intro.paragraph_format.space_after = Pt(12)
                  
                  # Add chapters
                  for chapter in novel_data["chapters"]:
                      # Add chapter title
                      chap_title = f"第{chapter['chapter_number_original']}章 {chapter['chapter_title']}"
                      doc.add_paragraph(chap_title, style='Heading1')
                      
                      # Add chapter content
                      content = chapter['content']
                      # Split content into paragraphs
                      paragraphs = content.split('\n')
                      for para in paragraphs:
                          if para.strip():  # Skip empty paragraphs
                              p = doc.add_paragraph(para)
                              p.paragraph_format.space_after = Pt(6)
                      
                      # Add page break between chapters except after last chapter
                      if chapter != novel_data["chapters"][-1]:
                          doc.add_page_break()
                  
                  doc.save(output_file_path)
                  return len(novel_data.get("chapters", [])) # Return chapter count saved
              except Exception as e:
                  print(f"Error saving DOCX for novel '{novel_name}': {e}")
                  raise
          # --- End save_novel_as_docx ---


          # --- unzip_and_process (Logs first 3, saves ALL) ---
          def unzip_and_process(zip_path, output_base_dir):
              """Unzips, processes text files, logs limited details, saves ALL chapters as DOCX."""
              results = {}
              temp_dir = None

              try:
                  # --- Extraction ---
                  zip_filename = os.path.basename(zip_path)
                  temp_dir_name = os.path.splitext(zip_filename)[0]
                  temp_parent_dir = os.path.dirname(zip_path)
                  temp_dir = os.path.join(temp_parent_dir, f"temp_extract_{temp_dir_name}")
                  if os.path.exists(temp_dir): shutil.rmtree(temp_dir, ignore_errors=True)
                  os.makedirs(temp_dir, exist_ok=True)
                  print(f"Extracting '{zip_filename}' to '{temp_dir}'...");
                  with zipfile.ZipFile(zip_path, 'r') as zip_ref: zip_ref.extractall(temp_dir)
                  print(f"Extraction complete.")

                  for root, _, files in os.walk(temp_dir):
                      for file in files:
                          if file.lower().endswith('.txt'):
                              file_path = os.path.join(root, file)
                              novel_name_base = os.path.splitext(file)[0]
                              print(f"\n--- Processing text file: {file} ---")

                              try:
                                  # 1. Read and Parse (includes sorting)
                                  content = read_file_with_fallback(file_path)
                                  if not content or content.isspace():
                                      results[file] = {"status": "skipped", "reason": "Empty file"}
                                      continue
                                  print(f"Parsing content for: {file}")
                                  novel_data = parse_novel_content(content) # Contains metadata + all sorted chapters

                                  # 2. Prepare for Logging
                                  display_name = novel_data.get("metadata", {}).get("name") or novel_name_base
                                  if display_name == "未知": display_name = novel_name_base
                                  all_chapters = novel_data.get("chapters", [])
                                  total_chapter_count = len(all_chapters)

                                  # 3. *** Log Limited Chapter Info (First 3) ***
                                  print(f"--- Chapter Index for: {display_name} (Displaying first 3/{total_chapter_count}) ---")
                                  if all_chapters:
                                      for i, chapter in enumerate(all_chapters):
                                          if i >= 3: # Limit console log
                                              print("  ...")
                                              break
                                          num_orig = chapter.get('chapter_number_original', 'N/A')
                                          num_numeric = chapter.get('chapter_number_numeric', 'N/A')
                                          title = chapter.get('chapter_title', 'No Title')
                                          print(f"  Index: {i}, Orig: '{num_orig}', Numeric: {num_numeric}, Title: \"{title}\"")
                                  else:
                                      print("  - No chapters were found or extracted.")
                                  print(f"--- End Chapter Index Log ---")

                                  # 4. *** Save FULL Data as DOCX ***
                                  print(f"Saving parsed data for '{display_name}' as DOCX...")
                                  novel_output_name = novel_name_base
                                  saved_chapter_count = save_novel_as_docx(novel_output_name, novel_data, output_base_dir)

                                  # 5. Record Success Summary (Report total chapters found/saved)
                                  first_chap_title = all_chapters[0].get("chapter_title", "N/A") if all_chapters else "N/A"
                                  last_chap_title = all_chapters[-1].get("chapter_title", "N/A") if all_chapters else "N/A"

                                  sanitized_output_name = re.sub(r'[<>:"/\\|?*]', '_', novel_output_name)[:200]

                                  results[file] = {
                                      "status": "success",
                                      "input_file": file,
                                      "output_file": f"{sanitized_output_name}.docx",
                                      "processed_novel_name": display_name,
                                      "chapters_found_and_saved": saved_chapter_count,
                                      "first_chapter_title": first_chap_title,
                                      "last_chapter_title_saved": last_chap_title
                                  }
                                  print(f"Successfully processed: '{file}' -> '{sanitized_output_name}.docx' (Saved {saved_chapter_count} chapters)")

                              # --- Error Handling ---
                              except ValueError as e: print(f"!!! Error processing file {file}: {e}"); results[file]={"status":"error","input_file":file,"error":f"File Error: {e}"}
                              except Exception as e: print(f"!!! Unexpected Error processing file {file}: {e}"); traceback.print_exc(); results[file]={"status":"error","input_file":file,"error":f"Unexpected Error: {e}"}
                          else: print(f"Skipping non-txt file: {file}")

              # --- Zip/Cleanup Handling ---
              except zipfile.BadZipFile: print(f"Error: '{zip_filename}' invalid zip."); results[zip_filename]={"status":"error","error":"Invalid zip"}
              except FileNotFoundError: print(f"Error: Zip file not found: {zip_path}"); results[zip_path]={"status":"error","error":"Zip not found"}
              except Exception as e: print(f"Zip processing error '{zip_filename}': {e}"); traceback.print_exc(); results[zip_filename or zip_path]={"status":"error","error":f"Zip Error: {e}"}
              finally:
                  if temp_dir and os.path.exists(temp_dir):
                       try: print(f"Cleaning temp dir: {temp_dir}"); shutil.rmtree(temp_dir)
                       except OSError as e: print(f"Warn: Could not remove temp dir {temp_dir}: {e}")
              return results
          # --- End unzip_and_process ---


          # --- process_novel_folder (Main Orchestration) ---
          def process_novel_folder():
              if not os.path.exists(NOVEL_DIR): print(f"Error: Input directory '{NOVEL_DIR}' does not exist."); return
              print(f"Starting novel processing...\nInput: '{os.path.abspath(NOVEL_DIR)}'\nOutput: '{os.path.abspath(OUTPUT_DIR)}'")
              zip_files = [f for f in os.listdir(NOVEL_DIR) if f.lower().endswith(".zip")]
              if not zip_files: print(f"No .zip files found in '{NOVEL_DIR}'."); return

              print(f"Found {len(zip_files)} zip file(s): {', '.join(zip_files)}")
              overall_summary = {}
              for zip_file_name in zip_files:
                  zip_file_path = os.path.join(NOVEL_DIR, zip_file_name)
                  print(f"\n================ Processing Zip: {zip_file_name} ================")
                  file_results = unzip_and_process(zip_file_path, OUTPUT_DIR)
                  overall_summary[zip_file_name] = file_results
                  print(f"\n--- Summary for Zip: {zip_file_name} ---")
                  if file_results:
                      success = sum(1 for r in file_results.values() if r.get("status") == "success")
                      errors = sum(1 for r in file_results.values() if r.get("status") == "error")
                      skipped = sum(1 for r in file_results.values() if r.get("status") == "skipped")
                      print(f"Items within zip: {len(file_results)} | Succeeded: {success} | Failed: {errors} | Skipped: {skipped}")
                      if errors > 0: print("Errors in:"); [print(f"  - {res.get('input_file', fn)}: {res.get('error')}") for fn, res in file_results.items() if res.get('status') == "error"]
                  else:
                       if zip_file_name in overall_summary and overall_summary[zip_file_name].get("status") == "error": print(f"Failed to process zip file itself: {overall_summary[zip_file_name].get('error')}")
                       else: print("No text files found or processed in this zip.")
                  print("------------------------------------")
              summary_file_path = os.path.join(OUTPUT_DIR, "processing_summary.json")
              print(f"\nSaving overall summary to: {summary_file_path}")
              try:
                  with open(summary_file_path, 'w', encoding='utf-8') as f: json.dump(overall_summary, f, ensure_ascii=False, indent=2)
                  print(f"Overall summary saved successfully.")
              except Exception as e: print(f"Error: Failed to save overall summary: {e}")
              print("\nNovel processing finished.")
          # --- End process_novel_folder ---

          # --- Main Execution ---
          process_novel_folder()
        shell: python

      - name: Commit and push changes
        if: github.ref == 'refs/heads/main'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          echo "Adding output directory changes..."
          git add output/
          echo "Current Git status:"
          git status
          if git diff --staged --quiet -- 'output/'; then
            echo "No changes detected in output/ directory. Nothing to commit."
          else
            echo "Changes detected in output/. Committing..."
            git commit -m "Update processed novels (DOCX format)"
            echo "Pushing changes..."
            git push
          fi
